---
title: "CVQA - Culturally-diverse Multilingual Visual Question Answering Benchmark"
authors:
  - name: "David Romero"
    # link: "FIRST AUTHOR PERSONAL LINK"
  - name: "Chenyang Lyu"
    # link: "SECOND AUTHOR PERSONAL LINK"
    # link: "THIRD AUTHOR PERSONAL LINK"

authors_main:
  - name: "Haryo Akbarianto Wibowo"
authors_annotators:
  - institution: MBZUAI
    name: Teresa Lynn
  - institution: MBZUAI
    name: Injy Hamed
  - institution: IIT Madras
    name: Aditya Nanda Kishore
  - institution: TU Darmstadt
    name: Aishik Mandal
  - institution: "Universidad de la Rep\xFAblica"
    name: Alina Dragonetti
  - institution: University of Michigan
    name: Artem Abzaliev
  - institution: MBZUAI
    name: Atnafu Lambebo Tonja
  - institution: .nan
    name: Bontu Fufa Balcha
  - institution: University of Cambridge
    name: Chenxi Whitehouse
  - institution: "Universidad Polit\xE9cnica Salesiana"
    name: Christian Salamea
  - institution: Samsung Research Philippines
    name: Dan John Velasco
  - institution: .nan
    name: David Ifeoluwa Adelani
  - institution: "Bretagne num\xE9rique"
    name: David Le Meur
  - institution: MBZUAI
    name: Emilio Villa-Cueva
  - institution: MBZUAI
    name: Fajri Koto
  - institution: .nan
    name: Fauzan Farooqui
  - institution: Federal University of Juiz de Fora
    name: Frederico Belcavello
  - institution: United Arab Emirates University / MBZUAI
    name: Ganzorig Batnasan
  - institution: The University of Melbourne
    name: Gisela Vallejo
  - institution: Dublin City University
    name: Grainne Caulfield
  - institution: "Universidad Nacional de C\xF3rdoba"
    name: Guido Ivetta
  - institution: NICT
    name: Haiyue Song
  - institution: EAII
    name: Henok Biadglign Ademtew
  - institution: "Universidad Nacional de C\xF3rdoba/CONICET"
    name: "Hern\xE1n Maina"
  - institution: AI Singapore
    name: Holy Lovenia
  - institution: Saarland University
    name: Israel Abebe Azime
  - institution: Samsung Research Philippines
    name: Jan Christian Blaise Cruz
  - institution: MBZUAI
    name: Jay Gala
  - institution: MBZUAI
    name: Jesus-German Ortiz-Barajas
  - institution: MBZUAI
    name: Jiahui Geng
  - institution: KAIST
    name: Jinheon Baek
  - institution: "Pontificia Universidad Cat\xF3lica de Chile"
    name: Jocelyn Dunstan Escudero
  - institution: MBZUAI
    name: Kumaranage Ravindu Yasas Nagasinghe
  - institution: "Universidad Nacional de C\xF3rdoba"
    name: Laura Alonso Alemany
  - institution: "Universidad Nacional de C\xF3rdoba/CONICET"
    name: Luciana Benotti
  - institution: .nan
    name: Luis Fernando D'Haro
  - institution: Federal University of Juiz de Fora
    name: Marcelo Viridiano
  - institution: "Universidad Polit\xE9cnica de Madrid"
    name: Marcos Estecha-Garitagoitia
  - institution: University of Stuttgart
    name: Maria Camila Buitrago Cabrera
  - institution: "Universidad Polit\xE9cnica de Madrid"
    name: "Mario Rodr\xEDguez-Cantelar"
  - institution: IKER, CNRS
    name: "M\xE9lanie Jouitteau"
  - institution: MBZUAI
    name: Mihail Mihaylov
  - institution: MBZUAI
    name: Mohamed Fazli Mohamed Imam
  - institution: MBZUAI
    name: Muhammad Farid Adilazuarda
  - institution: United Arab Emirates University
    name: Munkhjargal Gochoo
  - institution: United Arab Emirates University
    name: Munkh-Erdene Otgonbold
  - institution: .nan
    name: Naome Etori
  - institution: .nan
    name: Olivier Niyomugisha
  - institution: Millenium Institute Foundational Reseach on Data
    name: "Paula M\xF3nica Silva"
  - institution: .nan
    name: Pranjal Chitale
  - institution: IIT Madras
    name: Raj Dabre
  - institution: MBZUAI
    name: Rendi Chevi
  - institution: Brown University
    name: Ruochen Zhang
  - institution: ITB
    name: Ryandito Diandaru
  - institution: HKUST
    name: Samuel Cahyawijaya
  - institution: "Universidad de la Rep\xFAblica"
    name: "Santiago G\xF3ngora"
  - institution: KAIST
    name: Soyeong Jeong
  - institution: TU Darmstadt
    name: Sukannya Purkayastha
  - institution: MBZUAI
    name: Tatsuki Kuribayashi
  - institution: IIT Madras
    name: Thanmay Jayakumar
  - institution: Federal University of Juiz de Fora, CNPq
    name: Tiago Timponi Torrent
  - institution: MBZUAI
    name: Toqeer Ehsan
  - institution: KU Leuven
    name: Vladimir Araujo
  - institution: MBZUAI
    name: Yova Kementchedjhieva
  - institution: Skyline Highschool
    name: Zara Burzo
  - institution: The University of Melbourne
    name: Zheng Wei Lim
  - institution: Brown University
    name: Zheng-Xin Yong
  - institution: University of Michigan
    name: Oana Ignat
  - institution: University of Michigan
    name: Joan Nwatu
  - institution: University of Michigan
    name: Rada Mihalcea
authors_senior:
  - name: "Thamar Solorio"
  - name: "Alham Fikri Aji"

institution: "MBZUAI Core Team"
conference: "Submitted to a conference"
paperLink: "https://arxiv.org/abs/2406.05967"
arxivLink: ""
leaderboardLink: "https://eval.ai/web/challenges/challenge-page/2305/"
huggingfaceLink: "https://huggingface.co/datasets/afaji/cvqa"
teaserVideo: "static/videos/banner_video.mp4"
teaserDescription: "Aliquam vitae elit ullamcorper tellus egestas pellentesque."

youtubeEmbed: "https://www.youtube.com/embed/JkaxUblCGz0"

sectionsOrder:
  # - colaborators: 
  #   test: a
  - bibtex:
      bibtex: |
        @misc{romero2024cvqa,
              title={CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark}, 
              author={David Romero and Chenyang Lyu and Haryo Akbarianto Wibowo and Teresa Lynn and Injy Hamed and Aditya Nanda Kishore and Aishik Mandal and Alina Dragonetti and Artem Abzaliev and Atnafu Lambebo Tonja and Bontu Fufa Balcha and Chenxi Whitehouse and Christian Salamea and Dan John Velasco and David Ifeoluwa Adelani and David Le Meur and Emilio Villa-Cueva and Fajri Koto and Fauzan Farooqui and Frederico Belcavello and Ganzorig Batnasan and Gisela Vallejo and Grainne Caulfield and Guido Ivetta and Haiyue Song and Henok Biadglign Ademtew and Hernán Maina and Holy Lovenia and Israel Abebe Azime and Jan Christian Blaise Cruz and Jay Gala and Jiahui Geng and Jesus-German Ortiz-Barajas and Jinheon Baek and Jocelyn Dunstan and Laura Alonso Alemany and Kumaranage Ravindu Yasas Nagasinghe and Luciana Benotti and Luis Fernando D'Haro and Marcelo Viridiano and Marcos Estecha-Garitagoitia and Maria Camila Buitrago Cabrera and Mario Rodríguez-Cantelar and Mélanie Jouitteau and Mihail Mihaylov and Mohamed Fazli Mohamed Imam and Muhammad Farid Adilazuarda and Munkhjargal Gochoo and Munkh-Erdene Otgonbold and Naome Etori and Olivier Niyomugisha and Paula Mónica Silva and Pranjal Chitale and Raj Dabre and Rendi Chevi and Ruochen Zhang and Ryandito Diandaru and Samuel Cahyawijaya and Santiago Góngora and Soyeong Jeong and Sukannya Purkayastha and Tatsuki Kuribayashi and Thanmay Jayakumar and Tiago Timponi Torrent and Toqeer Ehsan and Vladimir Araujo and Yova Kementchedjhieva and Zara Burzo and Zheng Wei Lim and Zheng Xin Yong and Oana Ignat and Joan Nwatu and Rada Mihalcea and Thamar Solorio and Alham Fikri Aji},
              year={2024},
              eprint={2406.05967},
              archivePrefix={arXiv},
              primaryClass={cs.CV}
        }


---

<!-- {{% abstract %}}
Visual Question Answering~(VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA datasets and models are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA, a new Culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 28 countries on four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.{{% /abstract %}} -->

{{% text-section "./content/introduction.md" %}}
{{% text-section "./content/method.md" %}}
{{% text-section "./content/experiment-result.md" %}}
{{% text-section "./content/leaderboard.md" %}}


